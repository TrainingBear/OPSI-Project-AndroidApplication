{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "TUfAcER1oUS6"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Membuat model untuk sistem rekomendasi\n"
   ],
   "metadata": {
    "id": "PYDNzQ9WJ4zL"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "h6CRbtJ7KAi7"
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2q27gKz1H20"
   },
   "source": [
    "# **Membuat desain model CNN untuk klasifikasi jenis - jenis tanah**\n",
    "\n",
    " kami mengambil dataset tanah dari [kagle](https://www.kaggle.com/datasets/fuadkahfi/dataset-tanah), kita mendapat 8 sampel jenis tanah dan total 880 datasets\n",
    "\n",
    "untuk model arsitektur, kami meniru style [VGGNet CNN architecture](https://en.wikipedia.org/wiki/VGGNet). berikut model arsitektur yang kami buat:\n",
    "> 320x320x3 --> 320x320x32 Conv1+ReLU --> max pooling --> 160x160x64 Conv2+ReLU --> max pooling --> 80x80x128 Conv3+ReLU --> max pooling --> Dropout(0.4) -> Flatten --> 1x1x128 fullyconnected1+ReLU --> Dropout(0.4) --> 1x1x8 fullyconnected2+softmax\n",
    "\n",
    "dengan model tersebut, kami berhasil mendapatkan hasil yang luar biasa. hanya 4% gap antara training acc dan validation acc    \n",
    "Epoch 100/100                                        \n",
    "21/21 ━━━━━━━━━━━━━━━━━━━━ 5s 258ms/step - accuracy: 0.9476 - loss: 0.1651 - val_accuracy: 0.9091 - val_loss: 0.3009\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "berikut model yang kami buat :"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Install required libraries\n",
    "\n",
    "!pip install keras~=3.8.0 \\\n",
    "  matplotlib~=3.10.0 \\\n",
    "  numpy~=2.0.0 \\\n",
    "  pandas~=2.2.0\n",
    "\n",
    "print('\\n\\nAll requirements successfully installed.')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wNQJgIUxCZL7",
    "outputId": "f0191183-9761-41cf-e140-97e9358f70d1",
    "collapsed": true
   },
   "execution_count": 48,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: keras~=3.8.0 in /usr/local/lib/python3.12/dist-packages (3.8.0)\n",
      "Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
      "Requirement already satisfied: numpy~=2.0.0 in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
      "Requirement already satisfied: pandas~=2.2.0 in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (1.4.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (13.9.4)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (0.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (3.15.1)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (0.17.0)\n",
      "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (0.5.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (25.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas~=2.2.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas~=2.2.0) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras~=3.8.0) (4.15.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras~=3.8.0) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras~=3.8.0) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras~=3.8.0) (0.1.2)\n",
      "\n",
      "\n",
      "All requirements successfully installed.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Load dependencies\n",
    "\n",
    "import io\n",
    "\n",
    "# data\n",
    "import numpy as numpy\n",
    "import pandas as panda\n",
    "\n",
    "# machine learning\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# data visualization\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "\n",
    "# dataset\n",
    "import kagglehub\n",
    "from google.colab import drive"
   ],
   "metadata": {
    "id": "iNLA8xHaCwCD",
    "collapsed": true
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# @title Mempersiapkan dataset\n",
    "# drive.mount('/content/drive')\n",
    "# !unzip /content/drive/MyDrive/dataset_tanah.zip -d /content/dataset\n",
    "# dataset\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "path = kagglehub.dataset_download(\"fuadkahfi/dataset-tanah\")+\"/Tanah\"\n",
    "print(path)\n",
    "\n",
    "\n",
    "batch = 32 #@param\n",
    "p = 320 #@param\n",
    "l = 320 #@param\n",
    "\n",
    "# data set yang akan di latih dan validasi\n",
    "split = 0.2 # @param {\"type\":\"number\"}\n",
    "\n",
    "raw_training_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    path,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch,\n",
    "\n",
    "    validation_split=split,\n",
    "    subset=\"training\",\n",
    "    seed=1,\n",
    "\n",
    "    image_size=(p, l),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "training_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    path,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch,\n",
    "\n",
    "    validation_split=split,\n",
    "    subset=\"training\",\n",
    "    seed=1,\n",
    "\n",
    "    image_size=(p, l),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# Load 80% train, 20% temp (val + test)\n",
    "temp_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    path,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"int\",\n",
    "    color_mode=\"rgb\",\n",
    "    batch_size=batch,\n",
    "\n",
    "    validation_split=split,\n",
    "    subset=\"validation\",\n",
    "    seed=1,\n",
    "    image_size=(p, l),\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "temp_dataset = temp_dataset.shuffle(buffer_size=1000, seed=1)\n",
    "\n",
    "# Now split temp_dataset into val and test manually\n",
    "validation_dataset = temp_dataset.take(int(0.5 * len(temp_dataset)))\n",
    "test_dataset = temp_dataset.skip(int(0.5 * len(temp_dataset)))\n",
    "\n",
    "# training_dataset = raw_training_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "# test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "class_names = raw_training_dataset.class_names\n",
    "print(\"Class names:\", class_names)\n",
    "def count_dataset(dataset):\n",
    "    total = 0\n",
    "    for images, labels in dataset:\n",
    "        total += images.shape[0]  # batch size may be smaller on last batch\n",
    "    return total\n",
    "\n",
    "print(\"Training samples:\", count_dataset(training_dataset)/880*100)\n",
    "print(\"Validation samples:\", count_dataset(validation_dataset)/880*100)\n",
    "print(\"Test samples:\", count_dataset(test_dataset)/880*100)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLRodyOWPuhX",
    "outputId": "e3631e3b-2c4b-4363-c000-17816a3d6228"
   },
   "execution_count": 50,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using Colab cache for faster access to the 'dataset-tanah' dataset.\n",
      "/kaggle/input/dataset-tanah/Tanah\n",
      "Found 880 files belonging to 8 classes.\n",
      "Using 704 files for training.\n",
      "Found 880 files belonging to 8 classes.\n",
      "Using 704 files for training.\n",
      "Found 880 files belonging to 8 classes.\n",
      "Using 176 files for validation.\n",
      "Class names: ['01-Aluvial', '02-Andosol', '03-Entisol', '04-Humus', '05-Inceptisol', '06-Laterit', '07-Kapur', '08-Pasir']\n",
      "Training samples: 80.0\n",
      "Validation samples: 9.090909090909092\n",
      "Test samples: 9.090909090909092\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Membuat model\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "jenis = len(raw_training_dataset.class_names)\n",
    "print(jenis)\n",
    "droput = 4e-1 #@param\n",
    "regulasi = 1e-4 #@param\n",
    "\n",
    "augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomZoom((0.1, 0.1)),\n",
    "    tf.keras.layers.RandomRotation(0.04),\n",
    "    tf.keras.layers.RandomTranslation(0.05, 0.05),\n",
    "    # tf.keras.layers.RandomContrast(0.1),\n",
    "    # tf.keras.layers.RandomBrightness(0.1),\n",
    "    # AddGaussianNoise(0.01)\n",
    "])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(p, l, 3)),\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    augmentation,\n",
    "\n",
    "#    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "#    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "#    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(),\n",
    "\n",
    "    tf.keras.layers.Dropout(droput),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(regulasi)),\n",
    "    tf.keras.layers.Dropout(droput),\n",
    "    tf.keras.layers.Dense(jenis, activation='softmax', kernel_regularizer=regularizers.l2(regulasi))\n",
    "])\n",
    "\n"
   ],
   "metadata": {
    "id": "VRI3AURYXeg6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "64c3d739-095c-49ce-85a9-9b24e9cf07cb",
    "collapsed": true
   },
   "execution_count": 51,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Tahap Melatih model dan validaasi model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "rate = 4e-4 # @param {\"type\":\"number\"}\n",
    "epoch = 100 # @param {\"type\":\"integer\"}\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=rate),\n",
    "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    training_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epoch,\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1),\n",
    "        ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "        ]\n",
    ")\n",
    "\n",
    "# export model menjadi .tflite\n",
    "model.save('model.keras')\n",
    "convert = tf.lite.TFLiteConverter.from_keras_model(tf.keras.models.load_model('model.keras'))\n",
    "tflite_model = convert.convert()\n",
    "with open(\"Tanah.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n"
   ],
   "metadata": {
    "id": "e2EwaqHxmn3v",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "89aeab6c-4afe-44b3-a09b-4a23dfadfa34",
    "collapsed": true
   },
   "execution_count": 52,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 347ms/step - accuracy: 0.2671 - loss: 2.7420\n",
      "Epoch 1: val_loss improved from inf to 1.74780, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m20s\u001B[0m 816ms/step - accuracy: 0.2693 - loss: 2.7186 - val_accuracy: 0.4375 - val_loss: 1.7478 - learning_rate: 4.0000e-04\n",
      "Epoch 2/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 192ms/step - accuracy: 0.3861 - loss: 1.6234\n",
      "Epoch 2: val_loss improved from 1.74780 to 1.40222, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 333ms/step - accuracy: 0.3874 - loss: 1.6198 - val_accuracy: 0.5875 - val_loss: 1.4022 - learning_rate: 4.0000e-04\n",
      "Epoch 3/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 191ms/step - accuracy: 0.4918 - loss: 1.4398\n",
      "Epoch 3: val_loss did not improve from 1.40222\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 227ms/step - accuracy: 0.4908 - loss: 1.4402 - val_accuracy: 0.4792 - val_loss: 1.4266 - learning_rate: 4.0000e-04\n",
      "Epoch 4/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 192ms/step - accuracy: 0.4960 - loss: 1.3628\n",
      "Epoch 4: val_loss improved from 1.40222 to 1.25412, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 311ms/step - accuracy: 0.4964 - loss: 1.3618 - val_accuracy: 0.5625 - val_loss: 1.2541 - learning_rate: 4.0000e-04\n",
      "Epoch 5/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 192ms/step - accuracy: 0.5279 - loss: 1.2980\n",
      "Epoch 5: val_loss did not improve from 1.25412\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 230ms/step - accuracy: 0.5275 - loss: 1.2979 - val_accuracy: 0.5417 - val_loss: 1.4529 - learning_rate: 4.0000e-04\n",
      "Epoch 6/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 193ms/step - accuracy: 0.5959 - loss: 1.1352\n",
      "Epoch 6: val_loss improved from 1.25412 to 1.23514, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 309ms/step - accuracy: 0.5958 - loss: 1.1360 - val_accuracy: 0.5938 - val_loss: 1.2351 - learning_rate: 4.0000e-04\n",
      "Epoch 7/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 192ms/step - accuracy: 0.5872 - loss: 1.1406\n",
      "Epoch 7: val_loss improved from 1.23514 to 1.09419, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 275ms/step - accuracy: 0.5872 - loss: 1.1399 - val_accuracy: 0.5875 - val_loss: 1.0942 - learning_rate: 4.0000e-04\n",
      "Epoch 8/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 196ms/step - accuracy: 0.6027 - loss: 1.0772\n",
      "Epoch 8: val_loss improved from 1.09419 to 1.02173, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 281ms/step - accuracy: 0.6033 - loss: 1.0777 - val_accuracy: 0.6979 - val_loss: 1.0217 - learning_rate: 4.0000e-04\n",
      "Epoch 9/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 200ms/step - accuracy: 0.6031 - loss: 1.1036\n",
      "Epoch 9: val_loss did not improve from 1.02173\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 259ms/step - accuracy: 0.6027 - loss: 1.1055 - val_accuracy: 0.6250 - val_loss: 1.1976 - learning_rate: 4.0000e-04\n",
      "Epoch 10/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 223ms/step - accuracy: 0.5836 - loss: 1.1300\n",
      "Epoch 10: val_loss improved from 1.02173 to 0.91193, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m12s\u001B[0m 343ms/step - accuracy: 0.5842 - loss: 1.1288 - val_accuracy: 0.7000 - val_loss: 0.9119 - learning_rate: 4.0000e-04\n",
      "Epoch 11/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 195ms/step - accuracy: 0.6401 - loss: 1.0288\n",
      "Epoch 11: val_loss did not improve from 0.91193\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 231ms/step - accuracy: 0.6407 - loss: 1.0287 - val_accuracy: 0.7125 - val_loss: 0.9825 - learning_rate: 4.0000e-04\n",
      "Epoch 12/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 209ms/step - accuracy: 0.6687 - loss: 0.9259\n",
      "Epoch 12: val_loss did not improve from 0.91193\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 270ms/step - accuracy: 0.6683 - loss: 0.9263 - val_accuracy: 0.7500 - val_loss: 0.9442 - learning_rate: 4.0000e-04\n",
      "Epoch 13/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 193ms/step - accuracy: 0.6623 - loss: 1.0300\n",
      "Epoch 13: val_loss improved from 0.91193 to 0.87295, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 281ms/step - accuracy: 0.6630 - loss: 1.0271 - val_accuracy: 0.7125 - val_loss: 0.8730 - learning_rate: 4.0000e-04\n",
      "Epoch 14/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 197ms/step - accuracy: 0.7155 - loss: 0.8918\n",
      "Epoch 14: val_loss improved from 0.87295 to 0.86067, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 303ms/step - accuracy: 0.7157 - loss: 0.8905 - val_accuracy: 0.7125 - val_loss: 0.8607 - learning_rate: 4.0000e-04\n",
      "Epoch 15/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 193ms/step - accuracy: 0.7292 - loss: 0.8435\n",
      "Epoch 15: val_loss did not improve from 0.86067\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 230ms/step - accuracy: 0.7289 - loss: 0.8433 - val_accuracy: 0.7188 - val_loss: 0.9891 - learning_rate: 4.0000e-04\n",
      "Epoch 16/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 199ms/step - accuracy: 0.6957 - loss: 0.8615\n",
      "Epoch 16: val_loss improved from 0.86067 to 0.70649, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 312ms/step - accuracy: 0.6972 - loss: 0.8606 - val_accuracy: 0.7750 - val_loss: 0.7065 - learning_rate: 4.0000e-04\n",
      "Epoch 17/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 198ms/step - accuracy: 0.7799 - loss: 0.7067\n",
      "Epoch 17: val_loss did not improve from 0.70649\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 235ms/step - accuracy: 0.7788 - loss: 0.7095 - val_accuracy: 0.7396 - val_loss: 0.8332 - learning_rate: 4.0000e-04\n",
      "Epoch 18/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 197ms/step - accuracy: 0.7489 - loss: 0.8176\n",
      "Epoch 18: val_loss improved from 0.70649 to 0.65727, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 310ms/step - accuracy: 0.7486 - loss: 0.8170 - val_accuracy: 0.8125 - val_loss: 0.6573 - learning_rate: 4.0000e-04\n",
      "Epoch 19/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 193ms/step - accuracy: 0.7926 - loss: 0.6812\n",
      "Epoch 19: val_loss did not improve from 0.65727\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 230ms/step - accuracy: 0.7926 - loss: 0.6809 - val_accuracy: 0.7125 - val_loss: 0.8973 - learning_rate: 4.0000e-04\n",
      "Epoch 20/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 225ms/step - accuracy: 0.7889 - loss: 0.7199\n",
      "Epoch 20: val_loss improved from 0.65727 to 0.54685, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 337ms/step - accuracy: 0.7885 - loss: 0.7189 - val_accuracy: 0.8875 - val_loss: 0.5468 - learning_rate: 4.0000e-04\n",
      "Epoch 21/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 194ms/step - accuracy: 0.8123 - loss: 0.6394\n",
      "Epoch 21: val_loss did not improve from 0.54685\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 232ms/step - accuracy: 0.8124 - loss: 0.6399 - val_accuracy: 0.7625 - val_loss: 0.7822 - learning_rate: 4.0000e-04\n",
      "Epoch 22/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 230ms/step - accuracy: 0.7782 - loss: 0.6588\n",
      "Epoch 22: val_loss did not improve from 0.54685\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 266ms/step - accuracy: 0.7788 - loss: 0.6591 - val_accuracy: 0.6875 - val_loss: 1.1457 - learning_rate: 4.0000e-04\n",
      "Epoch 23/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 194ms/step - accuracy: 0.7900 - loss: 0.6627\n",
      "Epoch 23: val_loss did not improve from 0.54685\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 229ms/step - accuracy: 0.7904 - loss: 0.6618 - val_accuracy: 0.7375 - val_loss: 0.8876 - learning_rate: 4.0000e-04\n",
      "Epoch 24/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 227ms/step - accuracy: 0.7774 - loss: 0.6815\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 0.00019999999494757503.\n",
      "\n",
      "Epoch 24: val_loss did not improve from 0.54685\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 264ms/step - accuracy: 0.7780 - loss: 0.6807 - val_accuracy: 0.7917 - val_loss: 0.6399 - learning_rate: 4.0000e-04\n",
      "Epoch 25/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 195ms/step - accuracy: 0.8392 - loss: 0.5302\n",
      "Epoch 25: val_loss improved from 0.54685 to 0.39902, saving model to best_model.h5\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 309ms/step - accuracy: 0.8390 - loss: 0.5314 - val_accuracy: 0.8750 - val_loss: 0.3990 - learning_rate: 2.0000e-04\n",
      "Epoch 26/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 198ms/step - accuracy: 0.8315 - loss: 0.5551\n",
      "Epoch 26: val_loss did not improve from 0.39902\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 234ms/step - accuracy: 0.8324 - loss: 0.5536 - val_accuracy: 0.8125 - val_loss: 0.5938 - learning_rate: 2.0000e-04\n",
      "Epoch 27/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 196ms/step - accuracy: 0.8480 - loss: 0.5187\n",
      "Epoch 27: val_loss did not improve from 0.39902\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 233ms/step - accuracy: 0.8484 - loss: 0.5182 - val_accuracy: 0.8250 - val_loss: 0.6403 - learning_rate: 2.0000e-04\n",
      "Epoch 28/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 214ms/step - accuracy: 0.8898 - loss: 0.4384\n",
      "Epoch 28: val_loss did not improve from 0.39902\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m11s\u001B[0m 251ms/step - accuracy: 0.8896 - loss: 0.4393 - val_accuracy: 0.7500 - val_loss: 0.6776 - learning_rate: 2.0000e-04\n",
      "Epoch 29/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 199ms/step - accuracy: 0.8589 - loss: 0.4416\n",
      "Epoch 29: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "\n",
      "Epoch 29: val_loss did not improve from 0.39902\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 236ms/step - accuracy: 0.8590 - loss: 0.4420 - val_accuracy: 0.8438 - val_loss: 0.5006 - learning_rate: 2.0000e-04\n",
      "Epoch 30/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 196ms/step - accuracy: 0.8627 - loss: 0.4579\n",
      "Epoch 30: val_loss did not improve from 0.39902\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 233ms/step - accuracy: 0.8638 - loss: 0.4562 - val_accuracy: 0.8229 - val_loss: 0.7986 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 210ms/step - accuracy: 0.9135 - loss: 0.3628\n",
      "Epoch 31: val_loss did not improve from 0.39902\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 252ms/step - accuracy: 0.9133 - loss: 0.3631 - val_accuracy: 0.8542 - val_loss: 0.6628 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 196ms/step - accuracy: 0.9025 - loss: 0.3832\n",
      "Epoch 32: val_loss did not improve from 0.39902\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 232ms/step - accuracy: 0.9021 - loss: 0.3838 - val_accuracy: 0.8250 - val_loss: 0.4630 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 203ms/step - accuracy: 0.9030 - loss: 0.3491\n",
      "Epoch 33: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "\n",
      "Epoch 33: val_loss did not improve from 0.39902\n",
      "\u001B[1m22/22\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m10s\u001B[0m 239ms/step - accuracy: 0.9030 - loss: 0.3498 - val_accuracy: 0.9062 - val_loss: 0.4316 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 12 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saved artifact at '/tmp/tmpxfu9yfth'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 320, 320, 3), dtype=tf.float32, name='input_layer_12')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 8), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  132311673329168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132311673327632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132311673328208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132311673330896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132311673330128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132311673331280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132311673325904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132311673332624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132311673326672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  132311673333776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Extract all images and labels from test_dataset\n",
    "test_class_names = raw_training_dataset.class_names\n",
    "labels = list(range(len(test_class_names)))\n",
    "all_test_images = []\n",
    "all_test_labels = []\n",
    "for images, labels in test_dataset:\n",
    "    all_test_images.append(images.numpy())\n",
    "    all_test_labels.append(labels.numpy())\n",
    "\n",
    "all_test_images = np.concatenate(all_test_images, axis=0)\n",
    "all_test_labels = np.concatenate(all_test_labels, axis=0)\n",
    "\n",
    "# Assign y_true from the extracted labels\n",
    "y_true = all_test_labels\n",
    "\n",
    "# Predict classes using the extracted images\n",
    "y_pred_prob = model.predict(all_test_images)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "\n",
    "# 1. Classification report\n",
    "print(\"=== CLASSIFICATION REPORT ===\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    labels=labels,\n",
    "    target_names=test_class_names,\n",
    "    output_dict=True,\n",
    "    zero_division=0\n",
    "    ))\n",
    "\n",
    "# 2. Confusion Matrix\n",
    "cm = confusion_matrix(\n",
    "    y_true, y_pred,\n",
    "    labels=labels,\n",
    "    target_names=test_class_names,\n",
    "    )\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=test_class_names,yticklabels=test_class_names)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# 3. Training Accuracy/Loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title(\"Model Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Train\", \"Validation\"])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# For ROC curve, you need a binary classification or a multi-class one-vs-rest approach\n",
    "# Assuming it's a binary classification or a specific class vs rest for this line\n",
    "# If it's multi-class, you might need to adapt this part.\n",
    "# This line caused an error in previous run if there are more than 2 classes\n",
    "# fpr, tpr, _ = roc_curve(y_true, y_pred_prob[:, 1]) # This assumes binary classification or a specific class.\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plt.plot(fpr, tpr)\n",
    "# plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "# plt.title(f\"ROC Curve (AUC = {roc_auc:.3f})\")\n",
    "# plt.xlabel(\"False Positive Rate\")\n",
    "# plt.ylabel(\"True Positive Rate\")\n",
    "# plt.show()\n",
    "\n",
    "# df = panda.DataFrame(report).transpose()\n",
    "# print(df)\n",
    "# df.to_csv(\"precision_table.csv\", index=True)"
   ],
   "metadata": {
    "id": "xfyja47QS42b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "outputId": "119eb089-e71c-4394-fc44-54924aa05c11"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'raw_training_dataset' is not defined",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipython-input-2222755897.py\u001B[0m in \u001B[0;36m<cell line: 0>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;31m# Extract all images and labels from test_dataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m \u001B[0mtest_class_names\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mraw_training_dataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclass_names\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0mlabels\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest_class_names\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0mall_test_images\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'raw_training_dataset' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "mpqRXziW4rJs"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gb7qyhNL1yWt"
   },
   "source": [
    "# Image classification with TensorFlow Lite Model Maker with TensorFlow 2.18.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDABAblytltI"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/tutorials/model_maker_image_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m86-Nh4pMHqY"
   },
   "source": [
    "This notebook has been moved [here](https://www.tensorflow.org/lite/tutorials/model_maker_image_classification)."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "print(tf.__version__)"
   ],
   "metadata": {
    "id": "aygorWvwYyhY",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cddfd531-1a5d-4030-a9fd-2e1b49b8e883"
   },
   "execution_count": 54,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.19.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://colab.research.google.com/github/TrainingBear/OPSI-Project-AndroidApplication/blob/master/tensorflow_examples/lite/model_maker/demo/image_classification.ipynb#scrollTo=w7AdazbUlvDJ"
   ],
   "metadata": {
    "id": "w7AdazbUlvDJ"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "image_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
