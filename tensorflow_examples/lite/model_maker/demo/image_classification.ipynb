{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> Tambahkan blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "1LP4vJkj5BCx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "TUfAcER1oUS6"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Membuat model untuk sistem rekomendasi\n"
      ],
      "metadata": {
        "id": "PYDNzQ9WJ4zL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h6CRbtJ7KAi7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2q27gKz1H20"
      },
      "source": [
        "# **Membuat desain model CNN untuk klasifikasi jenis - jenis tanah**\n",
        "\n",
        " kami mengambil dataset tanah dari [kagle](https://www.kaggle.com/datasets/fuadkahfi/dataset-tanah), kita mendapat 8 sampel jenis tanah dan total 880 datasets\n",
        "\n",
        "untuk model arsitektur, kami meniru style [VGGNet CNN architecture](https://en.wikipedia.org/wiki/VGGNet). berikut model arsitektur yang kami buat:\n",
        "> 320x320x3 --> 320x320x32 Conv1+ReLU --> max pooling --> 160x160x64 Conv2+ReLU --> max pooling --> 80x80x128 Conv3+ReLU --> max pooling --> Dropout(0.4) -> Flatten --> 1x1x128 fullyconnected1+ReLU --> Dropout(0.4) --> 1x1x8 fullyconnected2+softmax\n",
        "\n",
        "dengan model tersebut, kami berhasil mendapatkan hasil yang luar biasa. hanya 4% gap antara training acc dan validation acc    \n",
        "Epoch 100/100                                        \n",
        "21/21 ━━━━━━━━━━━━━━━━━━━━ 5s 258ms/step - accuracy: 0.9476 - loss: 0.1651 - val_accuracy: 0.9091 - val_loss: 0.3009\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "berikut model yang kami buat :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install required libraries\n",
        "\n",
        "!pip install keras~=3.8.0 \\\n",
        "  matplotlib~=3.10.0 \\\n",
        "  numpy~=2.0.0 \\\n",
        "  pandas~=2.2.0\n",
        "\n",
        "print('\\n\\nAll requirements successfully installed.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNQJgIUxCZL7",
        "outputId": "f0191183-9761-41cf-e140-97e9358f70d1",
        "collapsed": true
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras~=3.8.0 in /usr/local/lib/python3.12/dist-packages (3.8.0)\n",
            "Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy~=2.0.0 in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas~=2.2.0 in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (0.17.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (0.5.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras~=3.8.0) (25.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas~=2.2.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas~=2.2.0) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras~=3.8.0) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras~=3.8.0) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras~=3.8.0) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras~=3.8.0) (0.1.2)\n",
            "\n",
            "\n",
            "All requirements successfully installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load dependencies\n",
        "\n",
        "import io\n",
        "\n",
        "# data\n",
        "import numpy as numpy\n",
        "import pandas as panda\n",
        "\n",
        "# machine learning\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "# data visualization\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "\n",
        "# dataset\n",
        "import kagglehub\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "iNLA8xHaCwCD",
        "collapsed": true
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Mempersiapkan dataset\n",
        "# drive.mount('/content/drive')\n",
        "# !unzip /content/drive/MyDrive/dataset_tanah.zip -d /content/dataset\n",
        "# dataset\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"fuadkahfi/dataset-tanah\")+\"/Tanah\"\n",
        "print(path)\n",
        "\n",
        "\n",
        "batch = 32 #@param\n",
        "p = 320 #@param\n",
        "l = 320 #@param\n",
        "\n",
        "# data set yang akan di latih dan validasi\n",
        "split = 0.2 # @param {\"type\":\"number\"}\n",
        "\n",
        "raw_training_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    path,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch,\n",
        "\n",
        "    validation_split=split,\n",
        "    subset=\"training\",\n",
        "    seed=1,\n",
        "\n",
        "    image_size=(p, l),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "training_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    path,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch,\n",
        "\n",
        "    validation_split=split,\n",
        "    subset=\"training\",\n",
        "    seed=1,\n",
        "\n",
        "    image_size=(p, l),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "\n",
        "# Load 80% train, 20% temp (val + test)\n",
        "temp_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    path,\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"int\",\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=batch,\n",
        "\n",
        "    validation_split=split,\n",
        "    subset=\"validation\",\n",
        "    seed=1,\n",
        "    image_size=(p, l),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "temp_dataset = temp_dataset.shuffle(buffer_size=1000, seed=1)\n",
        "\n",
        "# Now split temp_dataset into val and test manually\n",
        "validation_dataset = temp_dataset.take(int(0.5 * len(temp_dataset)))\n",
        "test_dataset = temp_dataset.skip(int(0.5 * len(temp_dataset)))\n",
        "\n",
        "# training_dataset = raw_training_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# validation_dataset = validation_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "# test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "class_names = raw_training_dataset.class_names\n",
        "print(\"Class names:\", class_names)\n",
        "def count_dataset(dataset):\n",
        "    total = 0\n",
        "    for images, labels in dataset:\n",
        "        total += images.shape[0]  # batch size may be smaller on last batch\n",
        "    return total\n",
        "\n",
        "print(\"Training samples:\", count_dataset(training_dataset)/880*100)\n",
        "print(\"Validation samples:\", count_dataset(validation_dataset)/880*100)\n",
        "print(\"Test samples:\", count_dataset(test_dataset)/880*100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLRodyOWPuhX",
        "outputId": "e3631e3b-2c4b-4363-c000-17816a3d6228"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'dataset-tanah' dataset.\n",
            "/kaggle/input/dataset-tanah/Tanah\n",
            "Found 880 files belonging to 8 classes.\n",
            "Using 704 files for training.\n",
            "Found 880 files belonging to 8 classes.\n",
            "Using 704 files for training.\n",
            "Found 880 files belonging to 8 classes.\n",
            "Using 176 files for validation.\n",
            "Class names: ['01-Aluvial', '02-Andosol', '03-Entisol', '04-Humus', '05-Inceptisol', '06-Laterit', '07-Kapur', '08-Pasir']\n",
            "Training samples: 80.0\n",
            "Validation samples: 9.090909090909092\n",
            "Test samples: 9.090909090909092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Membuat model\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "jenis = len(raw_training_dataset.class_names)\n",
        "print(jenis)\n",
        "droput = 4e-1 #@param\n",
        "regulasi = 1e-4 #@param\n",
        "\n",
        "augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.RandomZoom((0.1, 0.1)),\n",
        "    tf.keras.layers.RandomRotation(0.04),\n",
        "    tf.keras.layers.RandomTranslation(0.05, 0.05),\n",
        "    # tf.keras.layers.RandomContrast(0.1),\n",
        "    # tf.keras.layers.RandomBrightness(0.1),\n",
        "    # AddGaussianNoise(0.01)\n",
        "])\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(p, l, 3)),\n",
        "    tf.keras.layers.Rescaling(1./255),\n",
        "    augmentation,\n",
        "\n",
        "#    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "#    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "    tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "#    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "    tf.keras.layers.Conv2D(128, 3, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "    tf.keras.layers.Dropout(droput),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(regulasi)),\n",
        "    tf.keras.layers.Dropout(droput),\n",
        "    tf.keras.layers.Dense(jenis, activation='softmax', kernel_regularizer=regularizers.l2(regulasi))\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "VRI3AURYXeg6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c3d739-095c-49ce-85a9-9b24e9cf07cb",
        "collapsed": true
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tahap Melatih model dan validaasi model\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "\n",
        "rate = 4e-4 # @param {\"type\":\"number\"}\n",
        "epoch = 100 # @param {\"type\":\"integer\"}\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=rate),\n",
        "    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    training_dataset,\n",
        "    validation_data=validation_dataset,\n",
        "    epochs=epoch,\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6, verbose=1),\n",
        "        ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "        ]\n",
        ")\n",
        "\n",
        "# export model menjadi .tflite\n",
        "model.save('model.keras')\n",
        "convert = tf.lite.TFLiteConverter.from_keras_model(tf.keras.models.load_model('model.keras'))\n",
        "tflite_model = convert.convert()\n",
        "with open(\"Tanah.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n"
      ],
      "metadata": {
        "id": "e2EwaqHxmn3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89aeab6c-4afe-44b3-a09b-4a23dfadfa34",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 347ms/step - accuracy: 0.2671 - loss: 2.7420\n",
            "Epoch 1: val_loss improved from inf to 1.74780, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 816ms/step - accuracy: 0.2693 - loss: 2.7186 - val_accuracy: 0.4375 - val_loss: 1.7478 - learning_rate: 4.0000e-04\n",
            "Epoch 2/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.3861 - loss: 1.6234\n",
            "Epoch 2: val_loss improved from 1.74780 to 1.40222, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 333ms/step - accuracy: 0.3874 - loss: 1.6198 - val_accuracy: 0.5875 - val_loss: 1.4022 - learning_rate: 4.0000e-04\n",
            "Epoch 3/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 191ms/step - accuracy: 0.4918 - loss: 1.4398\n",
            "Epoch 3: val_loss did not improve from 1.40222\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 227ms/step - accuracy: 0.4908 - loss: 1.4402 - val_accuracy: 0.4792 - val_loss: 1.4266 - learning_rate: 4.0000e-04\n",
            "Epoch 4/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.4960 - loss: 1.3628\n",
            "Epoch 4: val_loss improved from 1.40222 to 1.25412, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 311ms/step - accuracy: 0.4964 - loss: 1.3618 - val_accuracy: 0.5625 - val_loss: 1.2541 - learning_rate: 4.0000e-04\n",
            "Epoch 5/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.5279 - loss: 1.2980\n",
            "Epoch 5: val_loss did not improve from 1.25412\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 230ms/step - accuracy: 0.5275 - loss: 1.2979 - val_accuracy: 0.5417 - val_loss: 1.4529 - learning_rate: 4.0000e-04\n",
            "Epoch 6/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.5959 - loss: 1.1352\n",
            "Epoch 6: val_loss improved from 1.25412 to 1.23514, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 309ms/step - accuracy: 0.5958 - loss: 1.1360 - val_accuracy: 0.5938 - val_loss: 1.2351 - learning_rate: 4.0000e-04\n",
            "Epoch 7/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 192ms/step - accuracy: 0.5872 - loss: 1.1406\n",
            "Epoch 7: val_loss improved from 1.23514 to 1.09419, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 275ms/step - accuracy: 0.5872 - loss: 1.1399 - val_accuracy: 0.5875 - val_loss: 1.0942 - learning_rate: 4.0000e-04\n",
            "Epoch 8/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 196ms/step - accuracy: 0.6027 - loss: 1.0772\n",
            "Epoch 8: val_loss improved from 1.09419 to 1.02173, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 281ms/step - accuracy: 0.6033 - loss: 1.0777 - val_accuracy: 0.6979 - val_loss: 1.0217 - learning_rate: 4.0000e-04\n",
            "Epoch 9/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 0.6031 - loss: 1.1036\n",
            "Epoch 9: val_loss did not improve from 1.02173\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 259ms/step - accuracy: 0.6027 - loss: 1.1055 - val_accuracy: 0.6250 - val_loss: 1.1976 - learning_rate: 4.0000e-04\n",
            "Epoch 10/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.5836 - loss: 1.1300\n",
            "Epoch 10: val_loss improved from 1.02173 to 0.91193, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 343ms/step - accuracy: 0.5842 - loss: 1.1288 - val_accuracy: 0.7000 - val_loss: 0.9119 - learning_rate: 4.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 195ms/step - accuracy: 0.6401 - loss: 1.0288\n",
            "Epoch 11: val_loss did not improve from 0.91193\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 231ms/step - accuracy: 0.6407 - loss: 1.0287 - val_accuracy: 0.7125 - val_loss: 0.9825 - learning_rate: 4.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 209ms/step - accuracy: 0.6687 - loss: 0.9259\n",
            "Epoch 12: val_loss did not improve from 0.91193\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 270ms/step - accuracy: 0.6683 - loss: 0.9263 - val_accuracy: 0.7500 - val_loss: 0.9442 - learning_rate: 4.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.6623 - loss: 1.0300\n",
            "Epoch 13: val_loss improved from 0.91193 to 0.87295, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 281ms/step - accuracy: 0.6630 - loss: 1.0271 - val_accuracy: 0.7125 - val_loss: 0.8730 - learning_rate: 4.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.7155 - loss: 0.8918\n",
            "Epoch 14: val_loss improved from 0.87295 to 0.86067, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 303ms/step - accuracy: 0.7157 - loss: 0.8905 - val_accuracy: 0.7125 - val_loss: 0.8607 - learning_rate: 4.0000e-04\n",
            "Epoch 15/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.7292 - loss: 0.8435\n",
            "Epoch 15: val_loss did not improve from 0.86067\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 230ms/step - accuracy: 0.7289 - loss: 0.8433 - val_accuracy: 0.7188 - val_loss: 0.9891 - learning_rate: 4.0000e-04\n",
            "Epoch 16/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - accuracy: 0.6957 - loss: 0.8615\n",
            "Epoch 16: val_loss improved from 0.86067 to 0.70649, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 312ms/step - accuracy: 0.6972 - loss: 0.8606 - val_accuracy: 0.7750 - val_loss: 0.7065 - learning_rate: 4.0000e-04\n",
            "Epoch 17/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step - accuracy: 0.7799 - loss: 0.7067\n",
            "Epoch 17: val_loss did not improve from 0.70649\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 235ms/step - accuracy: 0.7788 - loss: 0.7095 - val_accuracy: 0.7396 - val_loss: 0.8332 - learning_rate: 4.0000e-04\n",
            "Epoch 18/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step - accuracy: 0.7489 - loss: 0.8176\n",
            "Epoch 18: val_loss improved from 0.70649 to 0.65727, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 310ms/step - accuracy: 0.7486 - loss: 0.8170 - val_accuracy: 0.8125 - val_loss: 0.6573 - learning_rate: 4.0000e-04\n",
            "Epoch 19/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step - accuracy: 0.7926 - loss: 0.6812\n",
            "Epoch 19: val_loss did not improve from 0.65727\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 230ms/step - accuracy: 0.7926 - loss: 0.6809 - val_accuracy: 0.7125 - val_loss: 0.8973 - learning_rate: 4.0000e-04\n",
            "Epoch 20/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 225ms/step - accuracy: 0.7889 - loss: 0.7199\n",
            "Epoch 20: val_loss improved from 0.65727 to 0.54685, saving model to best_model.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 337ms/step - accuracy: 0.7885 - loss: 0.7189 - val_accuracy: 0.8875 - val_loss: 0.5468 - learning_rate: 4.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.8123 - loss: 0.6394\n",
            "Epoch 21: val_loss did not improve from 0.54685\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 232ms/step - accuracy: 0.8124 - loss: 0.6399 - val_accuracy: 0.7625 - val_loss: 0.7822 - learning_rate: 4.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step - accuracy: 0.7782 - loss: 0.6588\n",
            "Epoch 22: val_loss did not improve from 0.54685\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - accuracy: 0.7788 - loss: 0.6591 - val_accuracy: 0.6875 - val_loss: 1.1457 - learning_rate: 4.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step - accuracy: 0.7900 - loss: 0.6627\n",
            "Epoch 23: val_loss did not improve from 0.54685\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 229ms/step - accuracy: 0.7904 - loss: 0.6618 - val_accuracy: 0.7375 - val_loss: 0.8876 - learning_rate: 4.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m 7/22\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 196ms/step - accuracy: 0.7550 - loss: 0.7449"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='train acc')\n",
        "plt.plot(history.history['val_accuracy'], label='val acc')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Step 1: Get all images and labels from your test dataset into numpy arrays\n",
        "test_images_list = []\n",
        "test_labels_list = []\n",
        "for images_batch, labels_batch in test_dataset:\n",
        "    test_images_list.append(images_batch.numpy())\n",
        "    test_labels_list.append(labels_batch.numpy())\n",
        "\n",
        "y_true = numpy.concatenate(test_labels_list, axis=0)\n",
        "all_test_images = numpy.concatenate(test_images_list, axis=0)\n",
        "\n",
        "# Step 2: Get model predictions using the collected numpy array of images\n",
        "y_pred_probs = model.predict(all_test_images)\n",
        "y_pred = numpy.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Step 3: Get the class names from the training dataset (or any dataset directly from the directory)\n",
        "test_class_names = raw_training_dataset.class_names\n",
        "print(test_class_names)\n",
        "\n",
        "labels = list(range(len(test_class_names)))\n",
        "\n",
        "report = classification_report(\n",
        "    y_true, y_pred,\n",
        "    labels=labels,\n",
        "    target_names=test_class_names,\n",
        "    output_dict=True,\n",
        "    zero_division=0  # avoids divide-by-zero warnings\n",
        ")\n",
        "\n",
        "df = panda.DataFrame(report).transpose()\n",
        "print(df)\n",
        "df.to_csv(\"precision_table.csv\", index=True)"
      ],
      "metadata": {
        "id": "xfyja47QS42b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mpqRXziW4rJs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gb7qyhNL1yWt"
      },
      "source": [
        "# Image classification with TensorFlow Lite Model Maker with TensorFlow 2.18.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDABAblytltI"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/tutorials/model_maker_image_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/tensorflow/tensorflow/lite/g3doc/tutorials/model_maker_image_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m86-Nh4pMHqY"
      },
      "source": [
        "This notebook has been moved [here](https://www.tensorflow.org/lite/tutorials/model_maker_image_classification)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "aygorWvwYyhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/github/TrainingBear/OPSI-Project-AndroidApplication/blob/master/tensorflow_examples/lite/model_maker/demo/image_classification.ipynb#scrollTo=w7AdazbUlvDJ"
      ],
      "metadata": {
        "id": "w7AdazbUlvDJ"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "image_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}